{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/veroorli/ProjetProg/blob/master/TME1_Dataframes_collab_2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BDLE : Graphes en DataFrame"
      ],
      "metadata": {
        "id": "ppwDtMwygjsF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Données synthétiques sur la musique\n",
        "\n",
        "##### Description des données: \n",
        "\n",
        "- Fichier **data.csv** : contient des informations sur des chansons (trackId) interprétées par des artistes (artistId) et   écoutées par des utilisateurs (userId) à une date donnée par une estampille temporelle timestamp. Contient 260664 lignes.\n",
        "- Fichier **meta.csv**: contient les noms (champ 'Name') des chansons si type==track ou des artistes si type==artist.\n",
        "  'Artist' est le nom de l'artiste qui interprète la chanson si type==track ou le nom de l'artiste (même valeur que 'Name') si track==artist. 'Id' est l'identifiant d'une chanson ou d'un artiste. Contient 44319 lignes."
      ],
      "metadata": {
        "id": "4qO_BThNgxjY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lecture des données \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9Ga5tqvIhDFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construction du graphe\n"
      ],
      "metadata": {
        "id": "cVSOPqDs4QZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fonction utilisée par la suite pour calculer les poids des arcs\n",
        "# df: dataframe, source: nom de la colonne contenant les noeuds source des arcs\n",
        "# poids: poids initial avant normalisation, n: nombre maximum d'arcs à garder \n",
        "#pour chaque source\n",
        "from pyspark.sql.functions import row_number, sum\n",
        "from pyspark.sql import Window\n",
        "\n",
        "def calcul_poids(df, source, poids, n):\n",
        "    \n",
        "    window = Window.partitionBy(source).orderBy(col(poids).desc())\n",
        "    \n",
        "    # Sert juste à filtrer le nombre d'arc maximum pour chaque source\n",
        "    filterDF = df.withColumn(\"row_number\", row_number().over(window)) \\\n",
        "        .filter(col(\"row_number\") <= n) \\\n",
        "        .drop(col(\"row_number\")) \n",
        "        \n",
        "    # Somme des poids pour chaque source\n",
        "    tmpDF = filterDF.groupBy(col(source)).agg(sum(col(poids))\n",
        "    .alias(\"sum_\" + poids))\n",
        "   \n",
        "   # Normalisation des poids\n",
        "    finalDF = filterDF.join(tmpDF, source, \"inner\") \\\n",
        "        .withColumn(\"norm_\" + poids, col(poids) / col(\"sum_\" + poids)) \\\n",
        "        .cache()\n",
        "        \n",
        "    return finalDF"
      ],
      "metadata": {
        "id": "cyfdVl7t4RdR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Construction des liens pondérés entre utilisateurs et chansons \n",
        "* Construire un DataFrame userTrack à partir de data pour stocker les arcs entre utilisateurs et chansons. Pour chaque utilisateur (userId) on ajoute un arc vers une chanson (trackId) avec un poids égal au nombre total de fois que l'utilisateur à écouté la chanson. Utiliser la fonction calcul_poids pour garder pour chaque utilisateur les 100 chansons avec le poids le plus élevé et normaliser les poids des arcs gardés.   \n",
        "    \n",
        "* Affichage du résultat: garder uniquement les arcs qui ont les 5 plus grandes valeurs possibles des poids (utilisez la fonction rank() et la fenêtre over(window)). Afficher 20 lignes du résultat, en triant le résultat par ordre décroissant des poids, ensuite par ordre croissant des userId et des artistId."
      ],
      "metadata": {
        "id": "xx4_sb9B5eb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col, rank\n",
        "\n",
        "#nombre de fois que chaque utilisateurs écoute un track\n",
        "\n",
        "userTrack = data.groupby('userId', 'trackId').agg(first('artistId')\n",
        ".alias('artistId'), count('userId').alias('count'))\n",
        "\n",
        "#calculer le poids final en utilisant calcul_poids\n",
        "userTrack = calcul_poids(userTrack, 'userId', 'count', 100).persist() \n",
        "\n",
        "window = Window.partitionBy('userId').orderBy(col(\"norm_count\").desc())\n",
        "    \n",
        "userTrackList = userTrack.withColumn(\"position\", rank().over(window))\\\n",
        "  .where(col(\"position\")<6)\\\n",
        "  .orderBy('userId', 'artistId')\n",
        "  .select('userId', 'trackId', 'norm_count').take(20)\n",
        "\n",
        "for val in userTrackList:\n",
        "   print(\"%s %s %s\" % val)\n",
        "    \n",
        "userTrack.count() # résultat: 210675   "
      ],
      "metadata": {
        "id": "UjALrImW5kcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construction des liens pondérés entre utilisateurs et artistes\n",
        "* Construire un DataFrame userArtist à partir de data pour stocker les arcs entre utilisateurs et artistes. Pour chaque utilisateur (userId) on ajoute un arc vers un artiste (artistId) avec un poids égal au nombre total de fois que l'utilisateur à écouté des chansons de cet artiste. Utiliser la fonction calcul_poids pour garder pour chaque utilisateur au plus 100 artistes avec le poids le plus élevé et normaliser les poids des arcs gardés.   \n",
        "    \n",
        "* Affichage du résultat: garder uniquement les arcs qui ont les 5 plus grandes valeurs possibles des poids (utilisez la fonction rank() et la fenêtre over(window)). \n",
        "   Afficher 20 lignes du résultat, en triant le résultat par ordre décroissant des poids, ensuite par ordre croissant des userId et des artistId."
      ],
      "metadata": {
        "id": "E9doji9OFfDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#poids=nombre de fois qu'un utilisateurs a écouté des morceaux de cet artiste\n",
        "#regrouper les données par userId et artistId\n",
        "userArtist = data.groupby('userId', 'artistId').count()\n",
        "\n",
        "#calculer le poids final en utilisant calcul_poids\n",
        "userArtist = calcul_poids(userArtist, 'userId', 'count', 100).persist()  \n",
        "\n",
        "window = Window.partitionBy('userId').orderBy(col(\"norm_count\").desc())\n",
        "    \n",
        "userArtistList = userArtist.withColumn(\"position\", rank().over(window))\\\n",
        "  .where(col(\"position\")<6)\\\n",
        "  .orderBy('userId', 'artistId')\n",
        "  .select('userId', 'artistId', 'norm_count').take(20)\n",
        "\n",
        "for val in userArtistList:\n",
        "   print(\"%s %s %s\" % val)\n",
        "    \n",
        "userArtist.count() #résultat: 178419"
      ],
      "metadata": {
        "id": "2hnt69ccFnLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construction des liens pondérés entre artistes et chansons\n",
        "* Construire un DataFrame artistTrack à partir de data pour stocker les arcs entre artistes et chansons. Pour chaque artiste (artistId) on ajoute un arc vers une chanson (trackId) avec un poids égal au nombre total de fois que la chanson de cet artiste a été écoutée par tous les utilisateurs. Utiliser la fonction calcul_poids pour garder pour chaque artiste au plus 100 chansons avec le poids le plus élevé et normaliser les poids des arcs gardés.   \n",
        "    \n",
        "* Affichage du résultat: garder uniquement les arcs qui ont les 5 plus grandes valeurs possibles des poids (utilisez la fonction rank() et la fenêtre over(window)). \n",
        "   Afficher 20 lignes du résultat, en triant le résultat par ordre décroissant des poids, ensuite par ordre croissant des artistId et des trackId.* "
      ],
      "metadata": {
        "id": "BNRUOI5eF0_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# poids de l'arc: nombre de fois qu'un track de l'artiste a été écouté \n",
        "#par tous les utilisateurs\n",
        "artistTrack = data.groupby('artistId', 'trackId').count()\n",
        "\n",
        "#calculer le poids final en utilisant calcul_poids\n",
        "artistTrack = calcul_poids(artistTrack, 'artistId', 'count', 100).persist() \n",
        "\n",
        "window = Window.partitionBy(\"artistId\").orderBy(col(\"norm_count\").desc())\n",
        "    \n",
        "artistTrackList = artistTrack.withColumn(\"position\", rank().over(window))\\\n",
        "  .where(col(\"position\")<6)\\\n",
        "  .orderBy('artistId', 'trackId')\n",
        "  .select('artistId', 'trackId', 'norm_count').take(20)    \n",
        "\n",
        "for val in artistTrackList:\n",
        "   print(\"%s %s %s\" % val)\n",
        "    \n",
        "artistTrack.count() #résultat: 35408"
      ],
      "metadata": {
        "id": "rB31s1wrF5hS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construction des liens pondérés entre chansons\n",
        "* Construire un DataFrame trackTrack à partir de data pour stocker les arcs entre les chansons. Le poids total d'un arc entre trackId1 et trackId2 est le nombre total d'utilisateurs qui ont écouté à la fois trackId1 et trackId2 dans un laps de temps de 10 minutes (à noter que le graphe est non orienté, trackTrack contient à la fois une entrée pour (trackId1, trackId2) et une entrée pour (trackId2, trackId1)).   \n",
        "Utiliser la fonction calcul_poids pour garder pour chaque chanson au plus 100 chansons avec le poids le plus élevé et normaliser les poids gardés.   \n",
        "    \n",
        "* Affichage du résultat: garder uniquement les arcs qui ont les 5 plus grandes valeurs possibles des poids (utilisez la fonction rank() et la fenêtre over(window)). \n",
        "   Afficher 20 lignes du résultat, en triant le résultat par ordre décroissant des poids, ensuite par ordre croissant des artistId et des trackId."
      ],
      "metadata": {
        "id": "tyZIuE1yGDmb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Construire les couples de trackId écoutés par le même utilisateur\n",
        "data2 = data.selectExpr(\"userId as userId2\",\n",
        "\"trackId as trackId2\", \"artistId as artistId2\", \"timestamp as timestamp2\")\n",
        "trackTrack = data.join(data2, ((col(\"userId\")==col(\"userId2\"))\n",
        " & (col('trackId') != col(\"trackId2\"))))\n",
        ".where(abs(col('timestamp')-col('timestamp2')) < 600)\\\n",
        ".select('userId', \"trackId\", \"trackId2\")\n",
        "\n",
        "#pour chaque couple de trackId le nombre d'utilisateurs qui les ont écouté ensemble\n",
        "trackTrack = trackTrack.groupby(\"trackId\", \"trackId2\").count()\n",
        "\n",
        "#calculer le poids final en utilisant calcul_poids\n",
        "trackTrack =  calcul_poids(trackTrack, 'trackId', 'count', 100).persist()\n",
        "\n",
        "window = Window.partitionBy('trackId').orderBy(col(\"norm_count\").desc())\n",
        "\n",
        "trackTrackList = trackTrack.withColumn(\"position\", rank().over(window))\\\n",
        "    .where(col(\"position\")<6)\\\n",
        "    .orderBy('trackId').select('trackId', \"trackId2\", 'norm_count').take(20)   \n",
        "\n",
        "for val in trackTrackList:\n",
        "   print(\"%s %s %s\" % val)\n",
        "    \n",
        "trackTrack.count() #résultat: 136257  "
      ],
      "metadata": {
        "id": "IJBybGlyr0BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Construction du graphe final"
      ],
      "metadata": {
        "id": "s6szO3vKGWbF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construire un DataFrame graphe pour stocker tous les noeuds et tous les liens calculés précédemment. Le dataframe contiendra une colonne 'source' (identifiant du noeud source), une colonne 'destination' et une colonne 'poids'. \n",
        "Les colonnes 'source' et 'dest' contiennent à la fois des identifiants utilisateur, chanson et artiste. La colonne 'poids' contient les poids des arcs calculés à partir des poids précédents qui sont multipliés par les coefficients suivants:\n",
        "* Liens user->artist : 0.5\n",
        "* Liens user->track: 0.5\n",
        "* Liens track->track: 1\n",
        "* Liens artist->track: 1"
      ],
      "metadata": {
        "id": "jCB3eOxcGcOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "userTrack_g = userTrack.selectExpr\n",
        "(\"userId as source\", \"trackId as destination\", \"norm_count * 0.5 as poids\")\n",
        "userArtist_g = userArtist.selectExpr(\"userId as source\", \"artistId as destination\", \"norm_count * 0.5 as poids\")\n",
        "artistTrack_g = artistTrack.selectExpr(\"artistId as source\", \"trackId as destination\", \"norm_count as poids\")\n",
        "trackTrack_g = trackTrack.selectExpr(\"trackId as source\", \"trackId2 as destination\", \"norm_count as poids\")"
      ],
      "metadata": {
        "id": "tiDwZzLtWYo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graphe = userTrack_g.union(userArtist_g).union(artistTrack_g).union(trackTrack_g).persist()\n",
        "\n",
        "graphe.printSchema()\n",
        "\n",
        "graphe.count() #résultat: #560759"
      ],
      "metadata": {
        "id": "EI2e8fyeGqZa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calcul de recommendation de chansons avec PPR"
      ],
      "metadata": {
        "id": "fuI7LKmxHZi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En utilisant le calul de PageRank Personalisé, recommender à l'utilisateur avec l'identifiant 10 des chansons qu'il n'a pas écoutées.\n",
        "Rappel de la formule de mise à jour du score de recommendation à chaque itération du calcul:\n",
        "\n",
        "x[i] = alpha * u[i] + (1-alpha)* sum(xant[j]*poids[j][i])\n",
        "\n",
        "    - poids[j][i] : poids de l'arc entre j à i\n",
        "    - u[i]: valeur de personalisation, u[10]=1 et u[i]=0 si i !=10\n",
        "    - xant[j] : valeur du score du noeud j à l'itération précédente (x0[10]=1 et x0[i]=0 si i !=10)\n",
        "\n",
        "On considère alpha=0.15 et on effectue le calcul pour 5 itérations (maxiter=5)"
      ],
      "metadata": {
        "id": "ssDVSPK4HadQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calcul du vecteur de recommendation x"
      ],
      "metadata": {
        "id": "Ck2fUXL_HkaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pyspark.sql.functions import when\n",
        "\n",
        "user = 10\n",
        "d=0.85\n",
        "alpha=(1-d)\n",
        "maxiter = 5\n",
        "# Construire le vecteur d'importance initial\n",
        "x0  = spark.createDataFrame(pd.DataFrame([(user,1)], columns=[\"id\",\"rank\"]))\n",
        "\n",
        "print(\"Importance initiale\")\n",
        "x0.show()\n",
        "\n",
        "x = x0\n",
        "for iter in range(maxiter) :\n",
        "\n",
        "    # nextx = (1-alpha)* sum(xant[j]*poids[j][i])\n",
        "    nextx = x.join(graphe, x.id == graphe.source)\n",
        "    .selectExpr('destination', \"rank * poids as prod\")\n",
        "    .groupBy('destination').agg(sum('prod').alias('sum_prod'))\\\n",
        "    .select(\"destination\", (col('sum_prod') * d).alias(\"alpha_sum_prod\"))\n",
        "    \n",
        "    # x = alpha * u[i] + nextx\n",
        "    x = nextx.select(col('destination')\n",
        "    .alias(\"id\"), when(nextx.destination != user, nextx.alpha_sum_prod)\n",
        "    .otherwise(alpha + nextx.alpha_sum_prod).alias(\"rank\"))\n",
        "    \n",
        "    if x.filter(col('id')==user).count() == 0:\n",
        "      x = x.union(spark.createDataFrame\n",
        "                  (pd.DataFrame([(user, alpha)], columns=[\"id\",\"rank\"])))\n",
        "\n",
        "x.persist()\n",
        "print(\"Importance finale\")\n",
        "x.show()\n",
        "\n",
        "x.count() #résultat: 16168"
      ],
      "metadata": {
        "id": "37MJQ1Xza7LP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x.where(col('id').isin([841504, 844513, 847474, 853761, 854531, 856152, 856636]))\n",
        ".show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlxKsqjznR57",
        "outputId": "229b3e97-2953-49cd-96e3-e63d7638a72f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+--------------------+\n",
            "|    id|                rank|\n",
            "+------+--------------------+\n",
            "|854531|0.001554540512839496|\n",
            "|856636|9.373849924168476E-6|\n",
            "|841504|1.696264140221232...|\n",
            "|847474|3.237165813907451E-5|\n",
            "|853761|8.599485698358212E-4|\n",
            "|844513|5.834083375811894E-8|\n",
            "|856152|6.704348500324492E-8|\n",
            "+------+--------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Construction de la liste de chansons non écoutées recommendées\n",
        "Afficher les 10 chansons les plus recommendées que l'utilisateur n'a pas écoutées, avec leur score\n",
        "calculé précédemment"
      ],
      "metadata": {
        "id": "HjEpi7tAuGJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x2 = meta.selectExpr('Name', 'Artist', 'Id as id2')\n",
        "tracksrec = x.join(x2, x.id == x2.id2)\n",
        ".selectExpr('id', 'rank', 'Name', 'Artist')\n",
        "\n",
        "window = Window.orderBy(col(\"rank\").desc())\n",
        "\n",
        "tracksreckList = tracksrec.withColumn(\"position\", rank().over(window))\\\n",
        "    .where(col(\"position\")<=10)\\\n",
        "    .orderBy('position').select('id', \"rank\", 'Name', 'Artist').take(20)   \n",
        "       \n",
        "for val in tracksreckList:\n",
        "   print(\"%s %s %s %s\" % val)"
      ],
      "metadata": {
        "id": "vS3ikClIuKxx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c7372d-6d71-48cb-cca7-4eecfe3d819a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "839649 0.04804175618196066 Bazam Bekhand Artist: Mohsen Yeganeh\n",
            "828318 0.041461989256473215 Unapologetic Bitch Artist: Madonna\n",
            "960353 0.038525490989983785 Magneto And Titanium Man Artist: Wings\n",
            "960214 0.03797141587570513 Procession Artist: Queen\n",
            "955486 0.034484389884170796 Seni Bilen Biliyor Artist: Arif Susam\n",
            "855194 0.02795106930767553 Porke Artist: Pochill\n",
            "958924 0.02584262856304979 Pictures Artist: Gemafrei\n",
            "801772 0.02535247383791928 Celebration Day Artist: Led Zeppelin\n",
            "823737 0.02194303602829867 Dj Petros Artist: Bang La Decks\n",
            "807650 0.02194303602829867 Take care Artist: Imany\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcul des triangles\n",
        "\n",
        "Implémenter les différentes étapes de l'algorithme amélioré de calcul des triangles \n",
        "présenté en cours sur le graphe trackTrack construit précédemment."
      ],
      "metadata": {
        "id": "vBCj7dGTwF1V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trackTrack.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKoST9vtPbI-",
        "outputId": "e5443c01-dee6-4d61-e598-d3713feb230d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "136257"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Définir une fonction qui prend comme argument une liste d'utilisateurs triés \n",
        "# par ordre croissant (users) et retourne une liste de couples ordonnés d'utilisateurs\n",
        "\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "def parse_string(users):\n",
        "    \n",
        "    results = [[users[i], users[j]] for i in range(len(users)-1) \n",
        "                                    for j in range(i+1, len(users))]\n",
        "    \n",
        "    return results\n",
        "\n",
        "parse_string_udf = udf(parse_string, ArrayType(StringType()))"
      ],
      "metadata": {
        "id": "iSTjGZ1SwHCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Implémentation de l'algorithme de calcul de triangles\n",
        "\n",
        "from pyspark.sql.functions import collect_list, sort_array\n",
        "\n",
        "#Map1 - cours\n",
        "#Prendre en considération uniquement \n",
        "#les couples ordonnés (trackId, track1) (trackId < track1)  \n",
        "trackOrd = trackTrack.where(col('trackId') < col('trackId2'))\n",
        ".select('trackId', 'trackId2')\n",
        ".orderBy('trackId', 'trackId2')\n",
        "\n",
        "#Reduce 1 - cours: Construire pour chaque trackId la liste de couples \n",
        "#ordonnés de ses voisins\n",
        "# a) regrouper les lignes par trackId en construisant \n",
        "#la liste de voisins triés par ordre\n",
        "#    croissant (utiliser sort_array)\n",
        "neighbors = trackOrd.groupby('trackId')\n",
        ".agg(sort_array(collect_list('trackId2'))\n",
        ".alias('liste'))\n",
        "\n",
        "# b) utiliser la fonction définie précédemment pour retourner\n",
        "# la liste de couples de voisins\n",
        "couples= neighbors.withColumn('liste_couple',\n",
        "        explode(parse_string_udf(col('liste'))))\n",
        ".select('trackId', 'liste_couple')\n",
        "\n",
        "# Map2 + Reduce 2 - cours\n",
        "# prendre en considération uniquement les lignes telles \n",
        "#que les couples de voisins \n",
        "# construits précédemment existent également dans le graphe \n",
        "from pyspark.sql.functions import concat, lit, count, desc\n",
        "trackOrd_couple = trackOrd.withColumn('liste_couple',\n",
        "                                      concat(lit('['), col('trackId'), lit(', ')\n",
        "                                      , col('trackId2'), lit(']')))\n",
        ".select('liste_couple')\n",
        "\n",
        "liste =  couples.join(trackOrd_couple, on='liste_couple')\n",
        "\n",
        "# Calculer le nombre de triangles pour chaque utilisateur et \n",
        "# trier le résultat par le nombre de triangles décroissant\n",
        "triangles = liste.groupBy('trackId').count().orderBy(col('count').desc())\n",
        "\n",
        "print(triangles.count()) #résultat: 7156\n",
        "triangles.agg(sum(col(\"count\"))).show() #Résultat: 93400\n",
        "triangles.orderBy(col(\"count\").desc()).show()"
      ],
      "metadata": {
        "id": "rtl6tIWwwP9N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72a7a36d-bba3-451f-8ff5-29003d040984"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+--------------------+\n",
            "|trackId|               liste|\n",
            "+-------+--------------------+\n",
            "| 798256|    [923706, 954826]|\n",
            "| 798258|    [808254, 810685]|\n",
            "| 798261|[895481, 905671, ...|\n",
            "| 798290|[853797, 880442, ...|\n",
            "+-------+--------------------+\n",
            "only showing top 4 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calcul du plus court chemin\n",
        "\n",
        "Implémenter l'algorithme parallèle de calcul de plus court chemin d'origine fixée vers n'importe quelle destination"
      ],
      "metadata": {
        "id": "lrJNZLE-K4pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from graphframes.lib import AggregateMessages as AM\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "origin=10\n",
        "#Rassembler tous les noeuds  dans un seul DF vertices\n",
        "vertices = graphe.select(\"source\").distinct().withColumnRenamed(\"source\", \"id\")\n",
        ".union(graphe.select(\"dest\").distinct().withColumnRenamed(\"dest\", \"id\"))\n",
        ".distinct() \n",
        "# Vecteur des distances de tous les neouds par rapport à origin\n",
        "distances = vertices.withColumn(\"distance\", F.when(vertices[\"id\"] == origin, 0)\n",
        ".otherwise(float(\"inf\")))\n",
        "active  = spark.createDataFrame(pd.DataFrame([(origin,0)],\n",
        "                                             columns=[\"idA\",\"distanceA\"]))\n",
        "\n",
        "distances= AM.getCachedDataFrame(distances)\n",
        "\n",
        "while active.first():\n",
        "\n",
        "   ....\n",
        "   active=AM.getCachedDataFrame(...)\n",
        "   distances=AM.getCachedDataFrame((...)\n",
        "\n",
        "print(\"Distances finales:\")  \n",
        "distances.filter(col(\"distance\")!=float('inf')).orderBy(col(\"distance\")).show()"
      ],
      "metadata": {
        "id": "RqMWghpxK8Ds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "# Résultat attendu:\n",
        "\n",
        "Distances finales:\n",
        "+------+--------+\n",
        "|    id|distance|\n",
        "+------+--------+\n",
        "|    10|     0.0|\n",
        "|828318| 0.03125|\n",
        "|807650| 0.03125|\n",
        "|801772| 0.03125|\n",
        "|941064| 0.03125|\n",
        "|823737| 0.03125|\n",
        "|824440| 0.03125|\n",
        "|839649| 0.03125|\n",
        "|960214| 0.03125|\n",
        "|855194| 0.03125|\n",
        "|901153| 0.03125|\n",
        "|934050| 0.03125|\n",
        "|943645| 0.03125|\n",
        "|955486| 0.03125|\n",
        "|956604| 0.03125|\n",
        "|958924| 0.03125|\n",
        "|960353| 0.03125|\n",
        "|970381| 0.03125|\n",
        "|976111| 0.03125|\n",
        "|983989| 0.03125|\n",
        "+------+--------+\n",
        "only showing top 20 rows\n",
        "```"
      ],
      "metadata": {
        "id": "xn8SMrRMK8xu"
      }
    }
  ]
}